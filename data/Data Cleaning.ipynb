{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "import os\n",
    "import folium\n",
    "import branca.colormap as cm\n",
    "import geopy.distance as gpd\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_in_times(df, start_time, end_time):\n",
    "    return df[(df.timestamp > start_time) & (df.timestamp < end_time)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_timestamps(df):\n",
    "    timestamps = df.timestamp.unique()\n",
    "    hourly_timestamps = [timestamps[-i] for i in range(len(timestamps)) if i%6==1]\n",
    "    # This gives ASCENDING order 4:55am - 5:55am - .... 2:55am - 3:55am\n",
    "    # Do the same for ten_minute\n",
    "    ten_minute_timestamps = timestamps[::-1]\n",
    "    \n",
    "    return hourly_timestamps, ten_minute_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geodesic_distance(a, b):\n",
    "    return gpd.distance(a, b).m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_maps(df):\n",
    "    temp_df = df[df.timestamp == df.timestamp.unique()[0]]  # just need to get the location of each station\n",
    "    temp_df = temp_df['id'].reset_index()\n",
    "\n",
    "    # Used reset_index for the iterrows() function, which we want to start counting at 0\n",
    "    node_to_station_map = {}  # {node_id : station_id}\n",
    "    station_to_node_map = {}  # {station_id : node_id}\n",
    "    for index, row in temp_df.iterrows():\n",
    "        node_to_station_map[index] = int(row.id)\n",
    "        station_to_node_map[int(row.id)] = index\n",
    "\n",
    "    return node_to_station_map, station_to_node_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(df, station_to_node_map):\n",
    "    # Warning: quite slow (pdist)\n",
    "    temp_df = df[df.timestamp == df.timestamp.unique()[0]]  # just need to get the location of each station\n",
    "    temp_df = temp_df[['id','latitude','longitude']]\n",
    "    # Used reset_index for the iterrows() function, which we want to start counting at 0\n",
    "    matrix = np.empty((temp_df.shape[0], 2))\n",
    "\n",
    "    for index, row in temp_df.iterrows():\n",
    "        station_id = row.id  # id of station, needs mapping to id of node\n",
    "        node_id = station_to_node_map[station_id]\n",
    "        matrix[node_id,:] = [row.latitude, row.longitude]\n",
    "    \n",
    "    distance_matrix = pdist(matrix, geodesic_distance)\n",
    "    distance_matrix = squareform(distance_matrix)\n",
    "    \n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_station_states(dframe, unique_timestamps):\n",
    "    # Make a dictionary of the 'state' of each station for each timestamp\n",
    "    states = {}\n",
    "#     timestamp_dict = {}\n",
    "    for i, timestamp in enumerate(unique_timestamps):  # Should be in ASCENDING order\n",
    "        temp_df = dframe[dframe.timestamp == timestamp]\n",
    "        states[i] = {}\n",
    "        percentages = {}\n",
    "        for index, row in temp_df.iterrows():\n",
    "            percentages[row.id] = row.percent_full  # {station_id : percentage}\n",
    "\n",
    "        states[i]['timestamp'] = timestamp.astype(str)\n",
    "        states[i]['percentages'] = percentages  \n",
    "    \n",
    "    return states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_station_states_to_json(states, resolution, mode):\n",
    "    if mode == 'train':\n",
    "        filename = 'train_station_{}_states.json'.format(resolution)\n",
    "    elif mode == 'test':\n",
    "        filename = 'test_station_{}_states.json'.format(resolution)\n",
    "    elif mode == 'mini_train':\n",
    "        filename = 'mini_train_station_{}_states.json'.format(resolution)\n",
    "    elif mode == 'mini_test':\n",
    "        filename = 'mini_test_station_{}_states.json'.format(resolution)\n",
    "    elif mode == 'mini_total':\n",
    "        filename = 'mini_total_station_{}_states.json'.format(resolution)\n",
    "    \n",
    "    with open(filename, 'w') as fp:\n",
    "        json.dump(states, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_binary_labelings(station_states, station_to_node_map, percentage_threshold):\n",
    "    labelings = {}\n",
    "\n",
    "    for segment, data in station_states.items():\n",
    "        labeling = {}\n",
    "        for station, percentage in data['percentages'].items():\n",
    "            node = station_to_node_map[station]\n",
    "            labeling[node] = int(int(percentage >= percentage_threshold)*2 - 1)  #+1 or -1\n",
    "\n",
    "        labelings[int(segment)] = labeling\n",
    "\n",
    "    return labelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ids_of_switching_nodes(labelings):\n",
    "    switching_nodes = set()\n",
    "\n",
    "    previous_labeling = labelings[0]\n",
    "\n",
    "    for segment, labeling in labelings.items():\n",
    "        if segment == 0:  # skip first labeling\n",
    "            continue\n",
    "        for node, label in labeling.items():  # The labeling has the form {node_id: label}\n",
    "            if label != previous_labeling[node]:\n",
    "                switching_nodes.add(node)\n",
    "\n",
    "        previous_labeling = labeling\n",
    "\n",
    "    return list(switching_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_knn_matrix(distance_matrix, k):\n",
    "    argsorted = np.argsort(distance_matrix.view(np.ndarray), axis=1)[:,::1]\n",
    "\n",
    "    knn = np.zeros_like(distance_matrix, dtype=np.int8)\n",
    "\n",
    "    for i in range(argsorted.shape[0]):\n",
    "        for j in argsorted[i,:k]:\n",
    "            knn[i,j] = 1\n",
    "        knn[i,i] = 0\n",
    "    \n",
    "    return knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_labels_on_map(df, timestamps, threshold, location, color1, color2, radius):\n",
    "    mean_lat = np.mean(df['latitude'])\n",
    "    mean_long = np.mean(df['longitude'])\n",
    "    for timestamp in timestamps:\n",
    "        temp_df = df[df.timestamp == timestamp]\n",
    "        mapit = folium.Map( location=[ mean_lat, mean_long ], zoom_start=11 )\n",
    "        for index, row in temp_df.iterrows():\n",
    "            lat = row['latitude']\n",
    "            long = row['longitude']\n",
    "            if row['percent_full'] < threshold:\n",
    "                color=color1\n",
    "            else:\n",
    "                color=color2\n",
    "            folium.CircleMarker(radius=radius, location=[ lat, long], color=color ).add_to(mapit)\n",
    "        mapit.save( os.path.join(location,'{}.html'.format(str(timestamp))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all data, get 3 days, save to file (ONLY DO ONCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'rawdata'\n",
    "\n",
    "train_start_time = np.datetime64('2019-04-08T04:50:00')\n",
    "train_end_time = np.datetime64('2019-04-09T04:50:00')\n",
    "test_start_time = train_end_time\n",
    "test_end_time = np.datetime64('2019-04-11T04:50:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('rawdata/chicago_april.json', 'r') as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# train_and_test = []\n",
    "# train = []\n",
    "# test = []\n",
    "# for datum in data:\n",
    "#     dt = np.datetime64(datum['timestamp'])\n",
    "#     # add to train\n",
    "#     if dt >= train_start_time and dt < train_end_time:\n",
    "#         train.append(datum)\n",
    "#     # add to test\n",
    "#     if dt >= test_start_time and dt < test_end_time:\n",
    "#         test.append(datum)\n",
    "#     # add to train_and_test\n",
    "#     if dt >= train_start_time and dt < test_end_time:\n",
    "#         train_and_test.append(datum)\n",
    "\n",
    "# # Save json files\n",
    "# with open('rawdata/train.json', 'w') as f:\n",
    "#     json.dump(train, f)\n",
    "# with open('rawdata/test.json', 'w') as f:\n",
    "#     json.dump(test, f)\n",
    "# with open('rawdata/train_and_test.json', 'w') as f:\n",
    "#     json.dump(train_and_test, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 608 bike stations. \n",
    "\n",
    "The timestamps are every 10 minutes (roughly). We will use both ten-minute and hourly data, so for the latter we want to take every seventh timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set(color_codes=True)\n",
    "# plt.title('Distribution of percentage states of stations')\n",
    "# sns.distplot(df['percent_full'],bins=20)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 3 days of data, get timestamps and generate node ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3 consecutive days of training data. \n",
    "This has already been processed from the big json file, and has been saved into 3 json files:\n",
    "\n",
    "train.json - just the 24 hour train data\n",
    "\n",
    "test.json - just the 48 hour test data\n",
    "\n",
    "train_and_test.json - all 72 hours of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataframes from the csv files \n",
    "df_total = pd.read_json(os.path.join(data_dir, 'train_and_test.json'))\n",
    "df_train = pd.read_json(os.path.join(data_dir, 'train.json'))\n",
    "df_test = pd.read_json(os.path.join(data_dir, 'test.json'))\n",
    "\n",
    "# Get the 3 day timestamps\n",
    "total_hourly_timestamps, total_ten_minute_timestamps = get_timestamps(df_total)\n",
    "# Get the train timestamps\n",
    "train_hourly_timestamps, train_ten_minute_timestamps = get_timestamps(df_train)\n",
    "# Get the test timestamps\n",
    "test_hourly_timestamps, test_ten_minute_timestamps = get_timestamps(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "432"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.timestamp.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['In Service', 'Not In Service'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.status.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the station which has some 'Not in service issues'\n",
    "bad_id = df_total[df_total.status == 'Not In Service'].id.unique()[0]\n",
    "\n",
    "df_total = df_total[df_total.id != bad_id]\n",
    "df_train = df_train[df_train.id != bad_id]\n",
    "df_test = df_test[df_test.id != bad_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to convert this data into binary labelings, so that we can see which nodes switch and which don't.\n",
    "First we build node-station and station-node mappings, then get a dictionary of the state (percentage full) of each station on each timestamp, then turn those into binary labelings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the node-station and station-node dictionaries\n",
    "node_to_station_map, station_to_node_map = build_maps(df_total)\n",
    "\n",
    "# Get the station_states dictionary for 3 days data\n",
    "total_station_states = get_station_states(df_total, total_ten_minute_timestamps)\n",
    "\n",
    "# Turn this into a binarly labeling\n",
    "percentage_threshold = 50\n",
    "total_binary_labelings = generate_binary_labelings(station_states=total_station_states,\n",
    "                                                 station_to_node_map=station_to_node_map,\n",
    "                                                 percentage_threshold=percentage_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the binary labelings, get the ids of the ndoes that switch at some point\n",
    "total_switching_node_ids = get_ids_of_switching_nodes(total_binary_labelings)\n",
    "total_switching_station_ids = [node_to_station_map[node_id] for node_id in total_switching_node_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the ids of the nodes which switch during the 3 days, we need to create:\n",
    "\n",
    "new dataframes (train, test)\n",
    "\n",
    "new station_to_node_map and node_to_station_map\n",
    "\n",
    "states \n",
    "\n",
    "distance matrix\n",
    "\n",
    "knn graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New dataframes\n",
    "mini_df_train = df_train[df_train.id.isin(total_switching_station_ids)]\n",
    "mini_df_test = df_test[df_test.id.isin(total_switching_station_ids)]\n",
    "\n",
    "# New maps\n",
    "mini_node_to_station_map, mini_station_to_node_map = build_maps(mini_df_train)\n",
    "\n",
    "# New states\n",
    "mini_train_hourly_states = get_station_states(mini_df_train, train_hourly_timestamps)\n",
    "mini_train_ten_minute_states = get_station_states(mini_df_train, train_ten_minute_timestamps)\n",
    "mini_test_hourly_states = get_station_states(mini_df_test, test_hourly_timestamps)\n",
    "mini_test_ten_minute_states = get_station_states(mini_df_test, test_ten_minute_timestamps)\n",
    "\n",
    "# Distance matrix\n",
    "mini_distance_matrix = build_graph(mini_df_train, mini_station_to_node_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all the data in one for spine checks\n",
    "mini_df_total = df_total[df_total.id.isin(total_switching_station_ids)]\n",
    "mini_total_ten_minute_states = get_station_states(mini_df_total, total_ten_minute_timestamps)\n",
    "# Save test data (ten_minute)\n",
    "save_station_states_to_json(states=mini_total_ten_minute_states,\n",
    "                            resolution='ten_minute',\n",
    "                            mode='mini_total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the train/test labelings on the map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_labels_on_map(df=mini_df_train,\n",
    "                   timestamps=train_ten_minute_timestamps,\n",
    "                   threshold=50,\n",
    "                   location='plots/train',\n",
    "                   color1='orange',\n",
    "                   color2='black',\n",
    "                   radius=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the useful data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method considers self-neighboring, which we will remove later so really k=3\n",
    "k = 4\n",
    "knn = build_knn_matrix(distance_matrix=mini_distance_matrix, k=k)\n",
    "np.save('mini_{}_knn_matrix'.format(k-1), knn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the mini data\n",
    "# Save the pair-wise discance matrix \n",
    "np.save('mini_station_distance_matrix', mini_distance_matrix)\n",
    "\n",
    "# Save the station to node map { station_id : node_id }\n",
    "with open('mini_station_to_node_map.json', 'w') as fp:\n",
    "    json.dump(mini_station_to_node_map, fp)\n",
    "\n",
    "# Save the node to station map { node_id : station_id }\n",
    "with open('mini_node_to_station_map.json', 'w') as fp:\n",
    "    json.dump(mini_node_to_station_map, fp)\n",
    "    \n",
    "# Save training data (hourly)\n",
    "save_station_states_to_json(states=mini_train_hourly_states, \n",
    "                            resolution='hourly',\n",
    "                            mode='mini_train')\n",
    "\n",
    "# Save training data (ten_minute)\n",
    "save_station_states_to_json(states=mini_train_ten_minute_states,\n",
    "                            resolution='ten_minute',\n",
    "                            mode='mini_train')\n",
    "\n",
    "# Save test data (hourly)\n",
    "save_station_states_to_json(states=mini_test_hourly_states,\n",
    "                            resolution='hourly',\n",
    "                            mode='mini_test')\n",
    "\n",
    "# Save test data (ten_minute)\n",
    "save_station_states_to_json(states=mini_test_ten_minute_states,\n",
    "                            resolution='ten_minute',\n",
    "                            mode='mini_test')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
